import os
import sys
import tempfile
from pathlib import Path

import numpy as np

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
sys.path.insert(0, str(Path(__file__).parent / "src"))

from llmzip import Compressor, Decompressor, ContextStrategy, list_available_models
from llmzip.arithmetic import ArithmeticEncoder, ArithmeticDecoder
from llmzip.context import ContextManager, ContextStrategy
from llmzip.file_format import FileMetadata, save_compressed, load_compressed
from llmzip.preprocessing import Preprocessor


def test_list_models():
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    print("=" * 60)

    models = list_available_models()
    assert len(models) == 5, f"–û–∂–∏–¥–∞–ª–æ—Å—å 5 –º–æ–¥–µ–ª–µ–π, –ø–æ–ª—É—á–µ–Ω–æ {len(models)}"

    for m in models:
        print(f"  {m['key']:15s} | {m['display_name']:30s} | {m['family']}")

    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def test_arithmetic_coding():
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ")
    print("=" * 60)

    vocab_size = 100
    num_symbols = 50
    np.random.seed(42)

    symbols = np.random.randint(0, vocab_size, size=num_symbols)

    all_probs = []
    for _ in range(num_symbols):
        p = np.random.dirichlet(np.ones(vocab_size))
        all_probs.append(p.astype(np.float64))

    encoder = ArithmeticEncoder()
    encoder.create_encoder()

    for i in range(num_symbols - 1, -1, -1):
        probs = all_probs[i].astype(np.float32)
        probs /= probs.sum()
        encoder.encode_symbol(int(symbols[i]), probs)

    compressed = encoder.finish()
    print(f"  –°–∏–º–≤–æ–ª–æ–≤: {num_symbols}")
    print(f"  –°–∂–∞—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {len(compressed) * 4} –±–∞–π—Ç")

    decoder = ArithmeticDecoder(compressed)
    decoded = []
    for i in range(num_symbols):
        probs = all_probs[i].astype(np.float32)
        probs /= probs.sum()
        sym = decoder.decode_symbol(probs)
        decoded.append(sym)

    assert list(symbols) == decoded, "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!"
    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def test_preprocessing():
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
    print("=" * 60)

    preprocessor = Preprocessor()

    test_cases = [
        "Hello, World!",
        "Line1\nLine2\nLine3",
        "Mixed\r\nline\rendings\n",
        "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! Unicode —Ç–µ—Å—Ç üéâ",
        "",
    ]

    for text in test_cases:
        processed = preprocessor.preprocess(text)
        restored = preprocessor.reverse_preprocess(processed)
        expected = text.replace("\r\n", "\n").replace("\r", "\n")
        assert restored == expected, f"–ù–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {text!r}"
        print(f"  '{text[:30]}...' ‚Üí OK")

    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def test_context_manager():
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ")
    print("=" * 60)

    tokens = list(range(20))

    cm = ContextManager(max_context_length=5, strategy=ContextStrategy.SLIDING_WINDOW)

    ctx = cm.get_context(tokens, 0)
    assert ctx == [], f"–û–∂–∏–¥–∞–ª—Å—è [], –ø–æ–ª—É—á–µ–Ω–æ {ctx}"

    ctx = cm.get_context(tokens, 3)
    assert ctx == [0, 1, 2], f"–û–∂–∏–¥–∞–ª–æ—Å—å [0,1,2], –ø–æ–ª—É—á–µ–Ω–æ {ctx}"

    ctx = cm.get_context(tokens, 10)
    assert ctx == [5, 6, 7, 8, 9], f"–û–∂–∏–¥–∞–ª–æ—Å—å [5,6,7,8,9], –ø–æ–ª—É—á–µ–Ω–æ {ctx}"

    print("  –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ: OK")

    cm_block = ContextManager(max_context_length=5, strategy=ContextStrategy.BLOCK)

    ctx = cm_block.get_context(tokens, 7)
    assert ctx == [5, 6], f"–û–∂–∏–¥–∞–ª–æ—Å—å [5,6], –ø–æ–ª—É—á–µ–Ω–æ {ctx}"

    print("  –ë–ª–æ—á–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: OK")
    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def test_file_format():
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ .llmz")
    print("=" * 60)

    metadata = FileMetadata(
        model_key="gpt2",
        model_name="gpt2",
        vocab_size=50257,
        max_context_length=1024,
        context_strategy="sliding_window",
        encoding="utf-8",
        num_tokens=100,
        original_size=500,
        original_hash="abc123",
    )

    compressed_data = np.array([1, 2, 3, 4, 5], dtype=np.uint32)

    temp_path = os.path.join(tempfile.gettempdir(), "test_llmzip_full.llmz")

    try:
        save_compressed(temp_path, metadata, compressed_data)
        print(f"  –ó–∞–ø–∏—Å–∞–Ω–æ: {Path(temp_path).stat().st_size} –±–∞–π—Ç")

        loaded_meta, loaded_data = load_compressed(temp_path)

        assert loaded_meta.model_key == "gpt2"
        assert loaded_meta.vocab_size == 50257
        assert loaded_meta.original_hash == "abc123"
        assert list(loaded_data) == [1, 2, 3, 4, 5]

        print("  –ß—Ç–µ–Ω–∏–µ/–∑–∞–ø–∏—Å—å: OK")
    finally:
        if os.path.exists(temp_path):
            os.unlink(temp_path)

    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def test_compression_lossless(model_key: str = "gpt2"):
    print("\n" + "=" * 60)
    print(f"–¢–ï–°–¢: –°–∂–∞—Ç–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä—å (–º–æ–¥–µ–ª—å: {model_key})")
    print("=" * 60)

    test_text = (
        "The quick brown fox jumps over the lazy dog. "
        "In information theory, entropy measures the average uncertainty "
        "of a random variable. Shannon showed that the entropy of a source "
        "determines the theoretical limit of lossless compression. "
        "Large language models can serve as powerful probability estimators "
        "for arithmetic coding, potentially achieving compression ratios "
        "that surpass traditional methods like GZIP and BZIP2. "
        "This is a test of the LLMZip compression system. "
    ) * 3

    with tempfile.TemporaryDirectory() as tmpdir:
        input_path = os.path.join(tmpdir, "input.txt")
        compressed_path = os.path.join(tmpdir, "output.llmz")
        restored_path = os.path.join(tmpdir, "restored.txt")

        with open(input_path, "w") as f:
            f.write(test_text)

        original_size = os.path.getsize(input_path)
        print(f"  –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size} –±–∞–π—Ç")

        compressor = Compressor(model_key=model_key)
        comp_results = compressor.compress(input_path, compressed_path, verbose=True)

        compressed_size = os.path.getsize(compressed_path)
        print(f"  –°–∂–∞—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {compressed_size} –±–∞–π—Ç")
        print(f"  –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {comp_results['ratio']:.4f}")

        decompressor = Decompressor()
        decomp_results = decompressor.decompress(compressed_path, restored_path, verbose=True)

        with open(input_path, "r") as f:
            original = f.read()
        with open(restored_path, "r") as f:
            restored = f.read()

        assert original == restored, "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ù–ï —Å–æ–≤–ø–∞–¥–∞—é—Ç!"
        assert decomp_results["is_lossless"], "–•–µ—à–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!"

        print(f"\n  ‚úì LOSSLESS –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –∏—Å—Ö–æ–¥–Ω—ã–π == –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π")

    print("‚úì –ü—Ä–æ–π–¥–µ–Ω")


def run_all_tests():
    print("=" * 60)
    print("LLMZip v2.0 ‚Äî –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤")
    print("=" * 60)

    test_list_models()
    test_arithmetic_coding()
    test_preprocessing()
    test_context_manager()
    test_file_format()

    print("\n‚ö†Ô∏è  –°–ª–µ–¥—É—é—â–∏–π —Ç–µ—Å—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å GPT-2 (~500 MB).")
    print("    –î–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C.\n")

    try:
        test_compression_lossless("gpt2")
    except KeyboardInterrupt:
        print("\n  –ü—Ä–æ–ø—É—â–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")

    print("\n" + "=" * 60)
    print("–í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´")
    print("=" * 60)


if __name__ == "__main__":
    run_all_tests()
